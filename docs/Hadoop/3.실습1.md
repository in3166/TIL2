## HDFS 사용자 커멘드

- 데이터 디렉토리 생성: hadoop fs -mkdir /tmp/hive
- 리스트 보기: hadoop fs -ls /
- 데이터 넣기
  ```
  hadoop fs -mkdir -p "/home/in/hadoop1/test/"
  hadoop fs -put LISENCE.txt /home/in/hadoop1/test/
  hadoop fs -ls -h /home/in/hadoop1/test/
  ```
  
 - 로컬pc -> hadoop으로 복사: hadoop fs -copyFromLocal <localsrc> <hdfs destination>
 - hadoop -> 로컬: hadoop fs -get <src> <localdest>
 - 하둡 파일 내용 보기: hadoop fs -cat /path_to_file_in_hdfs |  hadoop fs -text /sample
 - 하둡 내 데이터 삭제:  hadoop fs -mv <src> <dest> 
 - 하둡 데이토 복제:  hadoop fs -cp <src> <dest>
  
 - replication 관리:  hadoop fs -setrep <rep> <path>
  ```
   hadoop fs -setrep 2 /newdata/sample
   - 해당 경로의 모든 데이터는 replication 2로 namenode가 관리
  ```
  
  - 파일 merge:  hadoop fs -getmerge <src> <localdest>
  
  ## HDFS 세이프 모드
  - 데이터 노드를 수정할 수 없는 상태
  - 데이터 노드 중 문제 상태(미싱 블록-Replication이 0인 경우)가 많은 경우 세이프모드에 들어감
  - 읽기 전용
  
  ## 커럽트 블록
  - 파일 자체가 깨짐
  - 유실 발생 경우 파일을 지우고 원본은 다시 하둡에 넣어줘야 함
  
  ## HDFS 휴지통 설정 및 명령어
  - 데이터 삭제 시 영구적 삭제하지 않도록 휴지통 설정
  - fs.trash.interval: 체크포인트를 삭제하는 시간 간격(분), 0이면 휴지통 기능 끔
  - fs.trash.checkpoint.interval: 체크포인트 확인 간격(분), 유효기간이 지난 체크포인트 삭제
  
  - core-site.xml 설정
  ```
  <property>
    <name>fs.trash.interval</name>
    <value>1440</value>
   </property>
   <property>
    <name>fs.trash.checkpoint.interval</name>
    <value>120</value>
   </property>
  ```
  
  - 명령어
  ```
  hadoop fs -expunge   // 휴지통 비움
  hadoop fs -rm -skipTrash /user/in/file   // 휴지통 이용하지 않고 삭제
  ```
  
 <img src="https://github.com/in3166/TIL/blob/main/Hadoop/3.JPG" width="90%">
 - 리포트 작성 명령어: hdfs dfsadmin -report
 - 디렉토리의 용량 Quota 설정: dfsadmin -setQuota
 <br/><br/>
 
 ## HDFS Balancers
 - 하둡 운영 중 다른 스펙의 데이터노드를 하나의 클러스터로 구성 
   - (18년도 도입 서버는 데이터노드 20TB, 19년도는 40TB -> 노드 하나에 디스크 capacity 불균형 발생)
 - 노드 간 디스크 크기가 다르거나 전체 데이터 밸런싱이 되지 않는 문제 발생 가능
 - 신규 데이터 노드를 추가하는 경우에도 발생 가능
   - 이 경우 NameNode 에서 데이터 적재량이 적은 노드를 우선적으로 선정하여 block을 추가, 이 때 특정 노드 부하 몰릴 수 있음.
 - 설정
   - hdfs-site.xml 설정 중 balancer 관련 설정
   - 대역폭 올리기: $HADOOP_HOME/bin/hdfs dfsadmin -setBalancerBandwidth 104857600 (대역폭 100MB 올림)
   
 ## WEB HDFS REST API 제공
  - HDFS는 REST API 이용하여 파일 조회, 생성, 수정, 삭제 기능 제공
  - 원격지에서 HDFS 내용 접근 가능
  - hdfs-site.xml 설정 필요
  ```
  <property>
   <name>dfs.webhdfs.enabled</name>
   <value>true</value>
  </property>
    <property>
   <name>dfs.namenode.http-address</name>
   <value>0.0.0.0:50070</value>
  </property>
  ```
  - 파일 리스트 확인 예제
  ```
  # /user/hadoop/ 위치 조회
  $ curl -s http://127.0.0.1:50070/webhdfs/v1/user/hadoop/?op=LISTSTATUS
  ```
  
  ## HDFS 암호화 - KMS (Key Management Server)
  - KeyProvider API 기반 암호화 키 관리 서버 (REST API 제공)
  
  
  # Hadoop 2.0
  - 마스터 서버 장애 해결
  - Standby NameNode 추가
    - 정보는 가져있고 동작하지 않고 있다가 Active NameNode가 중단됐을 때 Active로 올라간다.
    - **네임노드 이중화**
  
  ## 네임노드 고가용성
  - 분산 코디네이터 (zookeeper)
  - Journal Node 별도로 생성 (여러대 띄움 - 물리적 서버 노드)
    - Journaling: image 스냅샷과 로그를 함께 가지고 있는 방식
  - active NN와 NN standby가 JN을 공유하는 방식에서 이미지 파일을 어디 저장할 지에 대한 이슈
    1. 에디트로그 공유 방식1: NFS(Network File System)
      - edit 파일을 공유 스토리지를 이용하여 공유
      - 펜싱을 이용하여 하나의 네임노드만 에디트 로그를 기록
        - Fencing : Stanby NN이 전환될 때 처리되는 부분
      - Split Brain 위험 존재, 한계점
     
    2. 에디트로그 공유 방식2: Joural Node 그룹 사용
      - QJM(Quorum Journal Manager): NN 내부 구현 HDFS 전용 구현체, 저널 노드 그룹에서 동작, 각 에디트 로그는 전체 저널 노드에 동시에 사용
      
  ## HDFS Federation (거의 적용 안함 2000 NODE 이상될 때 고려)
   - 하나의 NN에서 관리하는 파일, 블록 개수가 많아지면 물리적 한계 존재 -> 해결
   - 파일, 디렉토리 정보를 가지는 네임스페이스와 블록의 정보를 가지는 블록 풀을 각 NN가 독립적으로 관리
   - 네임스페이스와 블록풀을 네임스페이스 볼륨이라 하는데 독립적 관리로 하나의 NN에 문제가 생겨도 다른 NN 영향x
   
   ## 아파치 주키퍼 (ZooKeeper)
   - 분산 시스템의 코디네이터: Active NN 선출 역할
     - 설정 관리
     - 분산 클러스터 관리
     - 명명 서비스
     - 분산 동기화
     - 분산 시스템에서 리더 선출
     - 중앙집중형 신뢰성있는 데이터 저장소
   
    - 기본 3-5의 서버를 하나의 클러스터로 구성 - 서버 앙상블
    - 여기에 여러 대의 노드를 붙여 작동 수행
    
    - 데이터 모델
      - / - /zoo - /zoo/duck | /zoo/goat | /zoo/cow
  
  
<출처>

https://tacademy.skplanet.com/live/player/onlineLectureDetail.action?seq=188
