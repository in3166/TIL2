### 경로
```
/home/in/Platform/hadoop 3.3.9
```
bin/hdfs namenode -format

- 실행: sbin/start-dfs.sh
  - localhost:9870
  - 하둡 분산 파일시스템의 상태를 보여주는 웹사이트 관리 도구
  - bin/hdfs namenode –format: 하둡 네임노드 포맷 해줘야 실행됨???
  
- 실행: sbin/start-yarn.sh
  - localhost:8088
  - 어플리케이션 실행에 대한 상태를 볼 수 있는 웹사이트 관리 도구
  
**분산 처리 vs 병렬 처리**
 - 분산 처리: 분산된 데이터를 처리
 - 병렬 처리: 데이터를 공유 저장, cpu core 수나 메모리를 늘려 처리
 
 ## Hadoop 특성
 - 수천, 수백대의 리눅스 기반 범용 서버들을 하나의 클러스터로 사용
 - Master-Slave 구조
   - Master: Name Node (분산파일시스템 HDFS의 마스터 데몬) / Job Tracker (분산파일시스템의 어플리케이션, job 처리/관리 마스터 데몬)
   - Slave: Data Node(데이터 저장/관리) + Task Tracer(업무 수행) (분산파일시스템 HDFS의 슬레이브 데몬)
   
  <img src="https://github.com/in3166/TIL/blob/main/Hadoop/1.JPG" width="90%">
  
 - 블록 단위 저장
   - 설정한 블록 단위보다 작으면 실제 크기로 저장
   - 128MB와 같이 큰 단위
     - 블록이 큰 이유는 탐색 비용을 최소할할 수 있기 때문
     - 블록이 크면 하드디스크에서 블록의 시작점을 탐색하는 데 걸리는 시간을 줄일 수 있고 네트워크 데이터 전송에 더 많은 시간 할당 가능
 
 - 복제본 유지로 신뢰성 보장 (기본 3개)
 - 높은 내고장성(Fault-Tolerance)
 - 데이터 처리의 지역성 보장
 <br/><br/>
   <img src="https://github.com/in3166/TIL/blob/main/Hadoop/2.JPG" width="90%">
 - 하나의 블록은 3대의 클러스터에 복제되어 자동 저장 (플랫폼 자체적 동작)
 - Data Node에서 Name Node로 Heartbeat를 주기적으로 보내고 장애 여부 판단 
   => 장애 서버의 데이터와 동일한 데이터를 가진 노드에 다른 정상적 서버에 새로운 카피본 저장하도록 지시 (자동 복구 3개 유지)
   
  - 마스터 노드가 죽는다면? 모든 서버에 장애 -> 하둡 2.0 / 하둡 3.0: 3배가아닌 2배정도의 저장용량만 필요하도록
  
  <br/><br/>
  ## 블록의 지역성
  - 여러 곳에 저장된 데이터에 대한 연산을 처리하고 싶을 때
    - 맵 리듀스 프로그래밍 작성 후 실행 시 실제 연산 데이터 측면 동작
      - 연산할 데이터 블록이 1, 2, 4번 서버에 나눠져 있으면 마스터는 Job을 1, 2 ,4번 서버에 할당
      - 슬레이브의 Task Tracker는 일단 자기 자신이 가지고 있는 데이터를 읽어서 처리 => Data Locality
 
 - 네트워크를 이용한 데이터 전송 시간 감소
 - 대용량 데이터 확인을 위한 디스크 탐색 시간 감소
 - 적절한 단위의 블록 크기를 이용한 CPU 처리시간 증가
 
   <br/><br/>
  ## 블록 캐싱
   - 데이터 노드 저장된 데이터 중 자주 읽는 블록은 블록 캐시라는 데이터 노드의 메모리에 명시적 캐싱
   - 파일 단위 캐싱도 가능, 조인에 사용되는 데이터들을 등록하여 읽기 성능 높힐 수 있음
   
  ## 네임노드의 역할
   - 전페 HDFS에 대한 Name Space 관리
   - Data Node로 부터 Block 리포트 받음
   - Data에 대한 Replication 유지 커맨더 역할 수행
   - 파일시스템 이미지 파일 관리 (fsimage: 스냅샷)
     - fsimage: 하둡 전체 파일시스템의 스냅샷 형태로 네임스페이스 구조를 저장한 파일 - /tmp/hadoop-.../dfs/name 에 저장됨
     - 하둡 네임노드는 전체 클러스터에 어떤 서버가 어떤 파일을 가지고 있는지 파악할 수 
   - 파일시스템에 대한 Edit Log 관리
   
  ## 보조 네임노드 (Secondary Name Node)
   - 실행 중 하둡의 변경사항 edits에 로그로 남음
   - 로그가 너무 많아지기 전에 fsimage와 병합해줘야 하는데 이때 보조 네임노드에서 수행
   - 장애 시 클러스터 동작엔 문제없지만 edits 로그가 계속 커지면서 오류 발생
   
  ## 데이터노드 역할 (Data Node)
   - 마스터 서버에 가지고 있는 데이터를 주기적 리포트
   - HDFS에 대한 지식 없음
   - 레이드 구성을 하지 않음 (JBOD 구성)
   - 블록 리포트: Name Node가 시작될 때, 그리고 로컬 파일시스템에 있는 모든 HDFS 블록들을 검사 후 정삭적인 블록의 목록을 만들어 NameNode에 전송
     <br/><br/>
 ## JAVA에서 HDFS 파일 읽기 샘플 코드
  ```JAVA
  FileSystem fileSystem = FileSystem.get(conf);
  Path path = new Path("/path/to/file.get");
  if(!fileSystem.exists(path)){
    System.out.println("File does not exists");
    return;
   }
   FSDataInputStream in = fileSystem.open(path);
   int numBytes = 0;
   while((numBytes = in.read(b)) > 0){
    System.out.println((char)numBytes));
   }
   in.close();
   out.close();
   fileSystem.close();
  ```

<출처>
 - https://tacademy.skplanet.com/live/player/onlineLectureDetail.action?seq=188
