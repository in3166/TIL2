# 비트

- 컴퓨터가 나타낼 수 있는 가장 작은 정보의 단위
- `(0 / 1)`: 신호가 있고 없고의 차이
- `2bit`: 2 * 2 - 4 가지 경우를 나타 낼 수 있다. (`3bit`: 8가지...)

# 바이트

- `1byte` = `8bit`: 256 가지의 경우를 가짐
- 컴퓨터 프로그래밍에서 최소 단위

- `ASCII 코드`
  - 문자는 미리 약속해 둔 테이블로 이진수로 표현 (1100001 = 'a')
  - `1byte`로 표현
  - 한계: 모든 언어를 표현할 수 없음

- `Unicode`
  - 모든 문자를 표현할 수 있는 코드
  - 심볼, 이모지 등도 표현
  - `2byte` 이상을 사용한다.

- 프로그래밍에서 변수 선언 시 어떤 데이터 타입이냐에 따라 메모리에 얼만큼 공간이 확보되는지 결정된다.
- C언어
  - `char`: 1byte (unsigned: [0, 255], signed[-127, 127])
  ...

## 바이트 단위

- `KB`: 1024 Byte, (2^10Byte)
- `MB`: 1024 KByte, (2^20Byte)
- `GB`: 1025 MByte, (2^30Byte)

## Text Encoding

- 현존하는 문자를 어떻게 binary 형태로 나타날지 규격을 약속
- 각 언어마다 인코딩 형식이 다양해서 사이트가 깨지는 등의 문제 발생

- UTF-8 (Unicode Transformation For mat(8bit))
  - `ASCII`(7bit이므로 1byte로 표현가능)와 `Unicode`(2byte-4byte로 표현가능) 모두 표현 가능
  - 가변길이 유니코드 인코딩
  -

<br><br><br>
<출처>

- <https://www.youtube.com/watch?v=5IRFJt1C5o4>
